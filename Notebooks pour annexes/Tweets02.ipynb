{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autres caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_tweet = pd.read_csv(\"Tweets/french_tweets.csv\")\n",
    "\n",
    "texte = data_tweet[\"text\"]\n",
    "\n",
    "#tous les modules nécessaires :\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import statistics\n",
    "import re\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(\"([A-Z]\\.[A-Z]?\\.?[0-9]?|[0-9]+[,.][0-9]+|[cdjls]'|qu'|[\\w'-]+|\\S)\")\n",
    "nb_instances= 0\n",
    "for i in texte:\n",
    "    l=len(tokenizer.tokenize(i))\n",
    "    nb_instances+=l\n",
    "\n",
    "nb_instances3= 0\n",
    "for i in texte:\n",
    "    longueur_text=len(sent_tokenize(i))\n",
    "    nb_instances3 += longueur_text\n",
    "\n",
    "nb_carac = 0\n",
    "for i in texte:\n",
    "    longueur_carac = len(i)\n",
    "    nb_carac += longueur_carac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# on reprend les comptes du début et on les adapte un peu\n",
    "\n",
    "liste_mots = []\n",
    "for i in texte:\n",
    "    l=tokenizer.tokenize(i)\n",
    "    for j in l:\n",
    "        liste_mots.append(j)\n",
    "# ici on fait une liste de tous les mots du corpus pour pouvoir l'utilisée ci-après\n",
    "\n",
    "somme_longueur = 0\n",
    "for i in liste_mots:\n",
    "    longueur = len(i)\n",
    "    somme_longueur += longueur\n",
    "# ici, on fait la somme de toutes les longueurs des mots du corpus pour pouvoir obtenir la moyenne\n",
    "\n",
    "moyenne_longueur = somme_longueur / len(liste_mots)\n",
    "\n",
    "nb_carac = 0\n",
    "for i in texte:\n",
    "    longueur_carac = len(i)\n",
    "    nb_carac += len(i)\n",
    "\n",
    "nb_phrases= 0\n",
    "for i in texte:\n",
    "    longueur_text=len(sent_tokenize(i))\n",
    "    nb_phrases += longueur_text\n",
    "\n",
    "taille_phrases = 0\n",
    "for i in texte:\n",
    "    longueur=sent_tokenize(i)\n",
    "    for j in longueur :\n",
    "        titi = len(j)\n",
    "        taille_phrases += titi\n",
    "\n",
    "moyenne_longueur2 = taille_phrases / nb_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "X_stylo = []#notre nouvelle matrice de description\n",
    "for text in data_tweet[\"text\"]:\n",
    "    mots=tokenizer.tokenize(text)\n",
    "    phrases = sent_tokenize(text)\n",
    "    NB_phrases = len(phrases)\n",
    "    NB_mots = len(mots)\n",
    "    NB_caracteres = len(text)\n",
    "    for x in mots :\n",
    "        moyenne_taille_mots = statistics.mean([len(x)])\n",
    "    moyenne_taille_phrases = NB_mots/nb_phrases\n",
    "    caracteristiques = [NB_phrases, NB_mots, NB_caracteres, moyenne_taille_mots, moyenne_taille_phrases]\n",
    "    X_stylo.append(caracteristiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[136523  94691]\n",
      " [117040 109764]]\n",
      "(array([0.53841846, 0.53686141]), array([0.59046165, 0.48395972]), array([0.56324042, 0.50903981]), array([231214, 226804]))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Positif       0.54      0.59      0.56    231214\n",
      "     Négatif       0.54      0.48      0.51    226804\n",
      "\n",
      "    accuracy                           0.54    458018\n",
      "   macro avg       0.54      0.54      0.54    458018\n",
      "weighted avg       0.54      0.54      0.54    458018\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexane.jouglar/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0 231214]\n",
      " [     0 226804]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Positif       0.00      0.00      0.00    231214\n",
      "     Négatif       0.50      1.00      0.66    226804\n",
      "\n",
      "    accuracy                           0.50    458018\n",
      "   macro avg       0.25      0.50      0.33    458018\n",
      "weighted avg       0.25      0.50      0.33    458018\n",
      "\n",
      "[[231214      0]\n",
      " [226804      0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Positif       0.50      1.00      0.67    231214\n",
      "     Négatif       0.00      0.00      0.00    226804\n",
      "\n",
      "    accuracy                           0.50    458018\n",
      "   macro avg       0.25      0.50      0.34    458018\n",
      "weighted avg       0.25      0.50      0.34    458018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nom_classes = [\"Positif\", \"Négatif\"]\n",
    "y = data_tweet[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stylo, y, test_size=0.3, random_state=0)\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "DT = DT.fit(X_train, y_train)\n",
    "y_pred = DT.predict(X_test)\n",
    "matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "print(matrice_confusion)\n",
    "stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "print(stats)\n",
    "report = classification_report(y_test, y_pred, target_names=nom_classes)\n",
    "print(report)\n",
    "\n",
    "y_spam = [1 for x in y_test]##une liste remplie de 1\n",
    "report = classification_report(y_test, y_spam, target_names=nom_classes)\n",
    "matrice_confusion = confusion_matrix(y_test, y_spam)\n",
    "print(matrice_confusion)\n",
    "print(report)\n",
    "\n",
    "y_ham = [0 for x in y_test]##une liste remplie de 0\n",
    "report = classification_report(y_test, y_ham, target_names=nom_classes)\n",
    "matrice_confusion = confusion_matrix(y_test, y_ham)\n",
    "print(matrice_confusion)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1526724\n",
      "5\n",
      "[[165676  65538]\n",
      " [ 63809 162995]]\n",
      "(array([0.72194697, 0.71322304]), array([0.71654831, 0.71866016]), array([0.71923751, 0.71593128]), array([231214, 226804]))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Positif       0.72      0.72      0.72    231214\n",
      "     Négatif       0.71      0.72      0.72    226804\n",
      "\n",
      "    accuracy                           0.72    458018\n",
      "   macro avg       0.72      0.72      0.72    458018\n",
      "weighted avg       0.72      0.72      0.72    458018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#**On combine le BOW et le stylométrique**\n",
    "V = CountVectorizer(ngram_range = (1,2) )\n",
    "X = V.fit_transform(data_tweet[\"text\"])\n",
    "##on crée une sparse matrix avec notre X_stylo\n",
    "from scipy.sparse import csr_matrix\n",
    "sparse_stylo = csr_matrix(X_stylo)\n",
    "print(sparse_stylo.shape[0])#NB lignes   -> instances\n",
    "print(sparse_stylo.shape[1])#Nb colonnes -> caractéristiques\n",
    "\n",
    "## on ale même nombre de ligne, on fait donc une conctaténation horizontale :\n",
    "from scipy.sparse import hstack\n",
    "X_fusion = hstack((X, sparse_stylo))\n",
    "\n",
    "#résultats :\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fusion, y, test_size=0.3, random_state=0)\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "DT = DT.fit(X_train, y_train)\n",
    "y_pred = DT.predict(X_test)\n",
    "matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "print(matrice_confusion)\n",
    "stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "print(stats)\n",
    "report = classification_report(y_test, y_pred, target_names=nom_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bons résultats 328671\n",
      "Erreurs: 129347\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "print('Erreurs: %d' % (y_test != y_pred).sum())\n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autres classifieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "[[155388  75826]\n",
      " [ 54102 172702]]\n",
      "(array([0.74174424, 0.69489957]), array([0.67205273, 0.76145923]), array([0.7051808 , 0.72665842]), array([231214, 226804]))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Positif       0.74      0.67      0.71    231214\n",
      "     Négatif       0.69      0.76      0.73    226804\n",
      "\n",
      "    accuracy                           0.72    458018\n",
      "   macro avg       0.72      0.72      0.72    458018\n",
      "weighted avg       0.72      0.72      0.72    458018\n",
      "\n",
      "Bons résultats 328090\n",
      "Erreurs: 129928\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LR = LogisticRegression()\n",
    "RDF = RandomForestClassifier()\n",
    "SVC = SVC()\n",
    "KN = KNeighborsClassifier()\n",
    "\n",
    "Liste = [LR, RDF, SVC, KN]\n",
    "\n",
    "for i in Liste:\n",
    "    i = i.fit(X_train, y_train)\n",
    "    y_pred = i.predict(X_test)\n",
    "    matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "    print(i)\n",
    "    print(matrice_confusion)\n",
    "    stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "    print(stats)\n",
    "    report = classification_report(y_test, y_pred, target_names=nom_classes)\n",
    "    print(report)\n",
    "    print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "    print('Erreurs: %d' % (y_test != y_pred).sum())\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En faisant varier les n-grammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_N in range(1, 2):\n",
    "  for max_N in range(min_N, 4):\n",
    "    V = CountVectorizer(ngram_range = (min_N, max_N), analyzer = \"char\")\n",
    "    X = V.fit_transform(data_spam[\"text\"])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "    print(f\"Ngram_range : ({min_N}, {max_N})\")\n",
    "    for i in Liste:\n",
    "        clf = i.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print('%s classifier : %.4f'%(nom, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forêt aléatoire : zoom sur quelques paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "    # On fait la somme de tous les cas où la valeur dans y_test est bien trouvée dans y_pred\n",
    "print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "print('Erreurs: %d' % (y_test != y_pred).sum())\n",
    "\n",
    "# ci-dessus : paramètres par défaut\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "for i in range(1,3):\n",
    "    clf = RandomForestClassifier(max_depth=i, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # On fait la somme de tous les cas où la valeur dans y_test est bien trouvée dans y_pred\n",
    "    print(\"Avec max_depth =\", i)\n",
    "    print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "    print('Erreurs: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En supprimant la ponctuation et les stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rappels pour la cellule ci-après (ces éléments de codes ont été exécutés plus avant)\n",
    "# LR = LogisticRegression()\n",
    "# RDF = RandomForestClassifier()\n",
    "# SVC = SVC()\n",
    "# KN = KNeighborsClassifier()\n",
    "# Liste = [LR, RDF, SVC, KN]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_ponctuation(chaine):\n",
    "    ponctuations = [\",\", \"'\", '\"', \"-\", \"\\.\"]\n",
    "    for stopword in ponctuations:\n",
    "        chaine = re.sub(f\" {stopword} \", \" \", chaine)\n",
    "    return chaine\n",
    "\n",
    "def remove_stopwords(chaine):\n",
    "    final_stopwords_list = stopwords.words('english')\n",
    "    s = chaine\n",
    "    for stopword in final_stopwords_list:\n",
    "        chaine = re.sub(f\" {stopword} \", \" \", chaine)\n",
    "    return chaine\n",
    "\n",
    "for pretraitement in [\"stopwords\", \"ponctuation\"]:\n",
    "    if pretraitement ==\"stopwords\":\n",
    "        liste_pretraite = [remove_stopwords(j) for j in texte]\n",
    "        X = V.fit_transform(liste_pretraite)\n",
    "    elif pretraitement ==\"ponctuation\":\n",
    "        liste_titres_pretraite = [remove_ponctuation(j) for j in texte]\n",
    "        X = V.fit_transform(liste_pretraite)    \n",
    "    else:\n",
    "        X = V.fit_transform(texte)\n",
    "    for split_size in [0.3]: #[0.1, 0.2, 0.3, 0.9]:\n",
    "        print(f\"Split_size : {split_size}, Pretraitement: {pretraitement}\")\n",
    "    #découpage train VS test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split_size, random_state=0)\n",
    "    ##classification\n",
    "        for i in Liste:\n",
    "            i = i.fit(X_train, y_train)\n",
    "            y_pred = i.predict(X_test)\n",
    "            matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "            print(i)\n",
    "            print(matrice_confusion)\n",
    "            stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "            print(stats)\n",
    "            report = classification_report(y_test, y_pred, target_names=nom_classes)\n",
    "            print(report)\n",
    "            print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "            print('Erreurs: %d' % (y_test != y_pred).sum())\n",
    "            print ('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "            print(\"-\"*10)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "liste_pretraite = [remove_ponctuation(j) for j in texte]\n",
    "X = V.fit_transform(liste_pretraite)    \n",
    "for split_size in [0.3]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split_size, random_state=0)\n",
    "\n",
    "for i in Liste:\n",
    "    i = i.fit(X_train, y_train)\n",
    "    y_pred = i.predict(X_test)\n",
    "    matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "    print(i)\n",
    "    print(matrice_confusion)\n",
    "    stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "    print(stats)\n",
    "    report = classification_report(y_test, y_pred, target_names=nom_classes)\n",
    "    print(report)\n",
    "    print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "    print('Erreurs: %d' % (y_test != y_pred).sum())\n",
    "    print ('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "    print(\"-\"*10)\n",
    "#print(corpus_sans_stopwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
