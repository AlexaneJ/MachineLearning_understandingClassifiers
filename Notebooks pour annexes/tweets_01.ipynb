{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeu de données 1 : Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 indique que le tweet est négatif\n",
    "# 1 indique qu'il est positif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture du jeu de données et comptes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_tweet = pd.read_csv(\"Tweets/french_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 1526724 tweets dans le jeu de données.\n"
     ]
    }
   ],
   "source": [
    "texte = data_tweet[\"text\"]\n",
    "print(\"Il y a\", len(texte), \"tweets dans le jeu de données.\")\n",
    "#for i in texte:\n",
    "    #print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(\"([A-Z]\\.[A-Z]?\\.?[0-9]?|[0-9]+[,.][0-9]+|[cdjls]'|qu'|[\\w'-]+|\\S)\")\n",
    "# tokenizer comprenant les mots avec une apostrophe (l', qu') et les ponctuations séparément\n",
    "# une telle tokenization nous permet d'avoir une bonne idée du nombre de token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 25979719 tokens en tout.\n"
     ]
    }
   ],
   "source": [
    "nb_instances= 0\n",
    "for i in texte:\n",
    "    l=len(tokenizer.tokenize(i))\n",
    "    nb_instances+=l\n",
    "print(\"Il y a\", nb_instances, \"tokens en tout.\") # nombre de tokens avec ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 23145150 tokens sans compter la ponctuation.\n"
     ]
    }
   ],
   "source": [
    "nb_instances2= 0\n",
    "for i in texte:\n",
    "    j = re.sub(\",|;|\\.\", \" \", i)\n",
    "    l2=len(tokenizer.tokenize(j))\n",
    "    nb_instances2 += l2\n",
    "print(\"Il y a\", nb_instances2, \"tokens sans compter la ponctuation.\") # nombre de tokens sans ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 2580261 phrases dans l'ensemble du jeu de données.\n"
     ]
    }
   ],
   "source": [
    "nb_instances3= 0\n",
    "for i in texte:\n",
    "    longueur_text=len(sent_tokenize(i))\n",
    "    nb_instances3 += longueur_text\n",
    "print(\"Il y a\", nb_instances3, \"phrases dans l'ensemble du jeu de données.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 119159266 caractères dans le jeu de données.\n"
     ]
    }
   ],
   "source": [
    "\n",
    " nb_carac = 0\n",
    "for i in texte:\n",
    "    longueur_carac = len(i)\n",
    "    nb_carac += longueur_carac\n",
    "print(\"Il y a\", nb_carac, \"caractères dans le jeu de données.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 771605 tweets négatifs au total et 755119 tweets positifs.\n"
     ]
    }
   ],
   "source": [
    "#771605 tweets négatifs\n",
    "positif = 1526724 - 771605\n",
    "print(\"Il y a 771605 tweets négatifs au total et\", positif, \"tweets positifs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_tweets = data_tweet[\"label\"]\n",
    "texte = data_tweet[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on récupère les instances (X) et les classes (y) et on les vectorise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_tweet[\"label\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "V = CountVectorizer(ngram_range = (1,2) )\n",
    "X = V.fit_transform(data_tweet[\"text\"])\n",
    "\n",
    "## séparer train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La taille du test est de 458017.2 items.\n"
     ]
    }
   ],
   "source": [
    "taille = 1526724 * 0.3\n",
    "print(\"La taille du test est de\", taille, \"items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Première évaluation : le perceptron, réseau de neurones simple, classifieur linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bons résultats 347500\n",
      "Erreurs: 110518\n"
     ]
    }
   ],
   "source": [
    "#classifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=0)\n",
    "ppn.fit(X_train, y_train)\n",
    "y_pred = ppn.predict(X_test)\n",
    "\n",
    "# On fait la somme de tous les cas où la valeur dans y_test est bien trouvée dans y_pred\n",
    "print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "print('Erreurs: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bon à première vue, les résultats ne sont pas concluants. On a 30% d'erreur..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bons résultats 348287\n",
      "Erreurs: 109731\n"
     ]
    }
   ],
   "source": [
    "ppn = Perceptron(eta0=0.2, random_state=0)\n",
    "ppn.fit(X_train, y_train)\n",
    "y_pred = ppn.predict(X_test)\n",
    "\n",
    "# On fait la somme de tous les cas où la valeur dans y_test est bien trouvée dans y_pred\n",
    "print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "print('Erreurs: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peu d'amélioration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec un eta0 = 1\n",
      "Bons résultats 343496\n",
      "Erreurs: 114522\n",
      "Avec un eta0 = 2\n",
      "Bons résultats 343859\n",
      "Erreurs: 114159\n",
      "Avec un eta0 = 3\n",
      "Bons résultats 344421\n",
      "Erreurs: 113597\n",
      "Avec un eta0 = 4\n",
      "Bons résultats 344082\n",
      "Erreurs: 113936\n",
      "Avec un eta0 = 5\n",
      "Bons résultats 343915\n",
      "Erreurs: 114103\n",
      "Avec un eta0 = 6\n",
      "Bons résultats 344421\n",
      "Erreurs: 113597\n",
      "Avec un eta0 = 7\n",
      "Bons résultats 343954\n",
      "Erreurs: 114064\n",
      "Avec un eta0 = 8\n",
      "Bons résultats 344082\n",
      "Erreurs: 113936\n",
      "Avec un eta0 = 9\n",
      "Bons résultats 343053\n",
      "Erreurs: 114965\n",
      "Avec un eta0 = 10\n",
      "Bons résultats 343915\n",
      "Erreurs: 114103\n",
      "Avec un eta0 = 11\n",
      "Bons résultats 342417\n",
      "Erreurs: 115601\n",
      "Avec un eta0 = 12\n",
      "Bons résultats 344421\n",
      "Erreurs: 113597\n",
      "Avec un eta0 = 13\n",
      "Bons résultats 344826\n",
      "Erreurs: 113192\n",
      "Avec un eta0 = 14\n",
      "Bons résultats 343954\n",
      "Erreurs: 114064\n",
      "Avec un eta0 = 15\n",
      "Bons résultats 343680\n",
      "Erreurs: 114338\n",
      "Avec un eta0 = 16\n",
      "Bons résultats 344814\n",
      "Erreurs: 113204\n",
      "Avec un eta0 = 17\n",
      "Bons résultats 346343\n",
      "Erreurs: 111675\n",
      "Avec un eta0 = 18\n",
      "Bons résultats 343053\n",
      "Erreurs: 114965\n",
      "Avec un eta0 = 19\n",
      "Bons résultats 343035\n",
      "Erreurs: 114983\n",
      "Avec un eta0 = 20\n",
      "Bons résultats 343915\n",
      "Erreurs: 114103\n",
      "Avec un eta0 = 21\n",
      "Bons résultats 343035\n",
      "Erreurs: 114983\n",
      "Avec un eta0 = 22\n",
      "Bons résultats 343356\n",
      "Erreurs: 114662\n",
      "Avec un eta0 = 23\n",
      "Bons résultats 340862\n",
      "Erreurs: 117156\n",
      "Avec un eta0 = 24\n",
      "Bons résultats 344421\n",
      "Erreurs: 113597\n",
      "Avec un eta0 = 25\n",
      "Bons résultats 344204\n",
      "Erreurs: 113814\n",
      "Avec un eta0 = 26\n",
      "Bons résultats 343709\n",
      "Erreurs: 114309\n",
      "Avec un eta0 = 27\n",
      "Bons résultats 344421\n",
      "Erreurs: 113597\n"
     ]
    }
   ],
   "source": [
    "for i in range (1, 28):\n",
    "    ppn = Perceptron(eta0=i, random_state=0)\n",
    "    ppn.fit(X_train, y_train)\n",
    "    y_pred = ppn.predict(X_test)\n",
    "\n",
    "# On fait la somme de tous les cas où la valeur dans y_test est bien trouvée dans y_pred\n",
    "    print(\"Avec un eta0 =\", i)\n",
    "    print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "    print('Erreurs: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Les données de classification : elles permettent d'évaluer la qualité de la classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.72      0.75    231214\n",
      "           1       0.73      0.78      0.76    226804\n",
      "\n",
      "    accuracy                           0.75    458018\n",
      "   macro avg       0.75      0.75      0.75    458018\n",
      "weighted avg       0.75      0.75      0.75    458018\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Positif       0.77      0.72      0.75    231214\n",
      "     Négatif       0.73      0.78      0.76    226804\n",
      "\n",
      "    accuracy                           0.75    458018\n",
      "   macro avg       0.75      0.75      0.75    458018\n",
      "weighted avg       0.75      0.75      0.75    458018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "nom_classes = [\"Positif\", \"Négatif\"]\n",
    "report = classification_report(y_test, y_pred, target_names=nom_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la précision est la proportion des items pertinents parmi l'ensemble des items proposés \n",
    "\n",
    "# le rappel est la proportion des items pertinents proposés parmi l'ensemble des items pertinents. \n",
    "\n",
    "# Vrai négatif : absent, absent\n",
    "# Vrai positif : présent, présent\n",
    "# Faux négatif : présent(dans la référence), absent(dans l'hypothèse)\n",
    "# Faux positif : absent, présent\n",
    "\n",
    "# support : nombre d'instances concernées\n",
    "\n",
    "# micro f-mesure : moyenne des F-mesure pondérée (une classe compte en fonction de sa taille)\n",
    "\n",
    "# macro f-mesure : moyenne des F-mesure de chaque classe (indépendamment de leur taille)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.77222251, 0.73391133]), array([0.7215134 , 0.78304175]), array([0.74600722, 0.75768093]), array([231214, 226804]))\n"
     ]
    }
   ],
   "source": [
    "##On peut avoir la même chose sous forme de liste python :\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "print(stats)\n",
    "##dans l'ordre les précisions pour chaque classe, puis les rappels ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[166824  64390]\n",
      " [ 49207 177597]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "print(matrice_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deuxième évaluation : Un arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[168697  62517]\n",
      " [ 63546 163258]]\n",
      "Bons résultats 331955\n",
      "Erreurs: 126063\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "DT = DT.fit(X_train, y_train)\n",
    "y_pred = DT.predict(X_test)\n",
    "\n",
    "# encore une matrice de confusion\n",
    "matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "print(matrice_confusion)\n",
    "\n",
    "print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "print('Erreurs: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73    231214\n",
      "           1       0.72      0.72      0.72    226804\n",
      "\n",
      "    accuracy                           0.72    458018\n",
      "   macro avg       0.72      0.72      0.72    458018\n",
      "weighted avg       0.72      0.72      0.72    458018\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.73      0.73      0.73    231214\n",
      "        spam       0.72      0.72      0.72    226804\n",
      "\n",
      "    accuracy                           0.72    458018\n",
      "   macro avg       0.72      0.72      0.72    458018\n",
      "weighted avg       0.72      0.72      0.72    458018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#Classification report permet de msurer l'exactitude d'une clissification selon plusieurs paramètres\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "nom_classes = [\"ham\", \"spam\"]\n",
    "report = classification_report(y_test, y_pred, target_names=nom_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Les paramètres random_state, max_depth, min_samples_split, min_samples_leaf, max_features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec la valeur par défaut de random state\n",
      "[[168794  62420]\n",
      " [ 63478 163326]]\n",
      "(array([0.72670834, 0.72349455]), array([0.73003365, 0.72011957]), array([0.7283672 , 0.72180312]), array([231214, 226804]))\n",
      "Bons résultats 332120\n",
      "Erreurs: 125898\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(\"Avec la valeur par défaut de random state\")\n",
    "for i in range(3):\n",
    "  DT = tree.DecisionTreeClassifier()\n",
    "  DT = DT.fit(X_train, y_train)\n",
    "  y_pred = DT.predict(X_test)\n",
    "  matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "  print(matrice_confusion)\n",
    "  stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "  print(stats)\n",
    "  print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "  print('Erreurs: %d' % (y_test != y_pred).sum())\n",
    "  print(\"--\"*10)\n",
    "\n",
    "print(\"En fixant random state\")\n",
    "for i in range(3):\n",
    "  DT = tree.DecisionTreeClassifier(random_state=0)\n",
    "  DT = DT.fit(X_train, y_train)\n",
    "  y_pred = DT.predict(X_test)\n",
    "  matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "  print(matrice_confusion)\n",
    "  stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "  print(stats)\n",
    "  print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "  print('Erreurs: %d' % (y_test != y_pred).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### print(\"On teste max_depth\")\n",
    "for i in range(1, 3):\n",
    "  DT = tree.DecisionTreeClassifier(max_depth=i)\n",
    "  DT = DT.fit(X_train, y_train)\n",
    "  y_pred = DT.predict(X_test)\n",
    "  matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "  print(\"Avec max_depth=\", i)\n",
    "  print(matrice_confusion)\n",
    "  stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "  print(stats)\n",
    "  print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "  print('Erreurs: %d' % (y_test != y_pred).sum())\n",
    "\n",
    "print(\"On teste min_samples_split:\")\n",
    "for i in range(1, 3):\n",
    "  DT = tree.DecisionTreeClassifier(max_depth=i)\n",
    "  DT = DT.fit(X_train, y_train)\n",
    "  y_pred = DT.predict(X_test)\n",
    "  matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "  print(\"Avec max_depth=\", i)\n",
    "  print(matrice_confusion)\n",
    "  stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "  print(stats)\n",
    "  print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "  print('Erreurs: %d' % (y_test != y_pred).sum())\n",
    "\n",
    "print(\"On teste min_samples_leaf:\")\n",
    "for i in range(1, 3):\n",
    "  DT = tree.DecisionTreeClassifier(max_depth=i)\n",
    "  DT = DT.fit(X_train, y_train)\n",
    "  y_pred = DT.predict(X_test)\n",
    "  matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "  print(\"Avec max_depth=\", i)\n",
    "  print(matrice_confusion)\n",
    "  stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "  print(stats)\n",
    "  print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "  print('Erreurs: %d' % (y_test != y_pred).sum())\n",
    "    \n",
    "print(\"On teste max_features:\")\n",
    "for i in range(1, 3):\n",
    "  DT = tree.DecisionTreeClassifier(max_depth=i)\n",
    "  DT = DT.fit(X_train, y_train)\n",
    "  y_pred = DT.predict(X_test)\n",
    "  matrice_confusion = confusion_matrix(y_test, y_pred)\n",
    "  print(\"Avec max_depth=\", i)\n",
    "  print(matrice_confusion)\n",
    "  stats = precision_recall_fscore_support(y_test, y_pred)\n",
    "  print(stats)\n",
    "  print('Bons résultats %d' % (y_test == y_pred).sum())\n",
    "  print('Erreurs: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"C'est ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
